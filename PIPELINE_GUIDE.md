# Data Mining Pipeline - HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

## ğŸ“‹ Tá»•ng Quan

Pipeline tá»± Ä‘á»™ng thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh data mining cho phÃ¢n vÃ¹ng sáº£n pháº©m e-commerce:

1. **Stage 1: Data Crawling** - Thu tháº­p dá»¯ liá»‡u tá»« Shopee, Tiki, Lazada
2. **Stage 2: Data Cleaning** - LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u
3. **Stage 3: Feature Engineering** - TrÃ­ch xuáº¥t vÃ  táº¡o features
4. **Stage 4: Labeling** - GÃ¡n nhÃ£n phÃ¢n vÃ¹ng sáº£n pháº©m
5. **Stage 5: Encoding** - MÃ£ hÃ³a dá»¯ liá»‡u vÃ  chia train/test

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### 1. Cháº¡y ToÃ n Bá»™ Pipeline (Bao Gá»“m Crawl)

```bash
python main.py --full
```

### 2. Cháº¡y ToÃ n Bá»™ Pipeline (Bá» Qua Crawl)

Náº¿u báº¡n Ä‘Ã£ cÃ³ dá»¯ liá»‡u crawl sáºµn:

```bash
python main.py --full --skip-crawl
```

### 3. Cháº¡y Má»™t Pháº§n Pipeline

Cháº¡y tá»« stage 2 Ä‘áº¿n stage 5 (clean â†’ encoding):

```bash
python main.py --partial --start 2 --end 5
```

### 4. Cháº¡y Tá»«ng Stage RiÃªng Láº»

#### Stage 1: Crawl Data

```bash
# Crawl táº¥t cáº£ platforms
python main.py --crawl

# Crawl chá»‰ Shopee vÃ  Tiki
python main.py --crawl --platforms shopee tiki

# Crawl vá»›i sá»‘ trang tÃ¹y chá»‰nh
python main.py --crawl --max-pages 10
```

#### Stage 2: Clean Data

```bash
python main.py --clean
```

#### Stage 3: Feature Engineering

```bash
# Vá»›i visualizations
python main.py --feature

# KhÃ´ng táº¡o visualizations
python main.py --feature --no-visualize
```

#### Stage 4: Labeling

```bash
python main.py --label
```

#### Stage 5: Encoding

```bash
python main.py --encode
```

## ğŸ“Š Luá»“ng Dá»¯ Liá»‡u

```
Raw Data (Shopee, Tiki, Lazada)
    â†“
data/preliminary/merged_preliminary_data.json
    â†“ [Stage 2: Clean]
data/clean/cleaned_merged_data.json
    â†“ [Stage 3: Feature Engineering]
data/transformation/engineered_features.json
    â†“ [Stage 4: Labeling]
data/transformation/labeled_data.json
    â†“ [Stage 5: Encoding]
data/transformation/encoded_data.json
data/transformation/encoders/ (encoder objects)
```

## âš™ï¸ TÃ¹y Chá»‰nh Cáº¥u HÃ¬nh

Báº¡n cÃ³ thá»ƒ chá»‰nh sá»­a cÃ¡c tham sá»‘ trong class `PipelineConfig` trong file `main.py`:

```python
# Crawl settings
self.max_pages = 5  # Sá»‘ trang tá»‘i Ä‘a cho má»—i category
self.sleep_min = 2  # Thá»i gian chá» tá»‘i thiá»ƒu (giÃ¢y)
self.sleep_max = 5  # Thá»i gian chá» tá»‘i Ä‘a (giÃ¢y)

# Labeling settings
self.use_model = True  # Sá»­ dá»¥ng ML model
self.prob_threshold = 0.70  # NgÆ°á»¡ng xÃ¡c suáº¥t
self.min_seed_per_class = 50  # Sá»‘ seed tá»‘i thiá»ƒu má»—i class
self.model_type = 'random_forest'  # Loáº¡i model

# Encoding settings
self.test_size = 0.2  # Tá»· lá»‡ test set (20%)
```

## ğŸ“ Logs

Pipeline tá»± Ä‘á»™ng ghi log vÃ o:
- **Console**: Hiá»ƒn thá»‹ real-time
- **File**: `pipeline.log` (trong thÆ° má»¥c gá»‘c)

## ğŸ¯ Káº¿t Quáº£

Sau khi cháº¡y xong pipeline, báº¡n sáº½ cÃ³:

### 1. Cleaned Data
- File: `data/clean/cleaned_merged_data.json`
- Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch, chuáº©n hÃ³a

### 2. Engineered Features
- File: `data/transformation/engineered_features.json`
- CÃ¡c features Ä‘Ã£ Ä‘Æ°á»£c táº¡o:
  - `popularity_score`: Äiá»ƒm phá»• biáº¿n (0-100)
  - `engagement_score`: Äiá»ƒm tÆ°Æ¡ng tÃ¡c (0-100)
  - `value_score`: Äiá»ƒm giÃ¡ trá»‹ (0-100)
  - `deal_quality_score`: Cháº¥t lÆ°á»£ng deal (0-100)
  - `trend_momentum`: Momentum xu hÆ°á»›ng
  - VÃ  nhiá»u features khÃ¡c...

### 3. Labeled Data
- File: `data/transformation/labeled_data.json`
- 4 nhÃ£n phÃ¢n vÃ¹ng:
  - ğŸ”¥ **Hot Trend**: Sáº£n pháº©m Ä‘ang viral
  - ğŸ† **Best Seller**: Sáº£n pháº©m bÃ¡n cháº¡y
  - ğŸ’° **Best Deal**: Æ¯u Ä‘Ã£i tá»‘t nháº¥t
  - ğŸ“¦ **Normal**: Sáº£n pháº©m thÃ´ng thÆ°á»ng

### 4. Encoded Data
- File: `data/transformation/encoded_data.json`
- Encoders: `data/transformation/encoders/`
- Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a, sáºµn sÃ ng cho ML models

## ğŸ”§ Xá»­ LÃ½ Lá»—i

### Lá»—i: "Merged file not found"
**Giáº£i phÃ¡p**: Äáº£m báº£o báº¡n Ä‘Ã£ cÃ³ dá»¯ liá»‡u trong `data/preliminary/` hoáº·c cháº¡y stage crawl trÆ°á»›c.

### Lá»—i: "Cleaned file not found"
**Giáº£i phÃ¡p**: Cháº¡y stage 2 (clean) trÆ°á»›c khi cháº¡y stage 3.

### Lá»—i: "Engineered file not found"
**Giáº£i phÃ¡p**: Cháº¡y stage 3 (feature engineering) trÆ°á»›c khi cháº¡y stage 4.

### Lá»—i: "Labeled file not found"
**Giáº£i phÃ¡p**: Cháº¡y stage 4 (labeling) trÆ°á»›c khi cháº¡y stage 5.

## ğŸ’¡ Tips

1. **Láº§n Ä‘áº§u cháº¡y**: Sá»­ dá»¥ng `--full` Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline
2. **Thá»­ nghiá»‡m**: Sá»­ dá»¥ng `--max-pages 2` Ä‘á»ƒ crawl Ã­t dá»¯ liá»‡u hÆ¡n
3. **Debug**: Cháº¡y tá»«ng stage riÃªng láº» Ä‘á»ƒ dá»… kiá»ƒm tra
4. **Production**: TÄƒng `max_pages` Ä‘á»ƒ cÃ³ nhiá»u dá»¯ liá»‡u hÆ¡n

## ğŸ“ Há»— Trá»£

Náº¿u gáº·p váº¥n Ä‘á», kiá»ƒm tra:
1. File log: `pipeline.log`
2. Console output
3. Äáº£m báº£o táº¥t cáº£ dependencies Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t: `pip install -r requirements.txt`

## ğŸ“ VÃ­ Dá»¥ Workflow

### Workflow 1: Láº§n Äáº§u Cháº¡y (Full Pipeline)
```bash
# Cháº¡y toÃ n bá»™ tá»« crawl Ä‘áº¿n encoding
python main.py --full --max-pages 3
```

### Workflow 2: ÄÃ£ CÃ³ Dá»¯ Liá»‡u Raw
```bash
# Bá» qua crawl, cháº¡y tá»« clean Ä‘áº¿n encoding
python main.py --full --skip-crawl
```

### Workflow 3: Chá»‰ Cáº­p Nháº­t Features vÃ  Labels
```bash
# Cháº¡y tá»« stage 3 Ä‘áº¿n 5
python main.py --partial --start 3 --end 5
```

### Workflow 4: Thá»­ Nghiá»‡m Labeling
```bash
# Chá»‰ cháº¡y labeling Ä‘á»ƒ xem káº¿t quáº£
python main.py --label
```

### Workflow 5: Crawl ThÃªm Dá»¯ Liá»‡u
```bash
# Crawl thÃªm tá»« Lazada
python main.py --crawl --platforms lazada --max-pages 10

# Sau Ä‘Ã³ merge vÃ  clean láº¡i
python main.py --partial --start 2 --end 5
```

---

**Happy Data Mining! ğŸš€**
